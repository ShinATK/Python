![[Pasted image 20230220161516.png]]
- 假设我们将权重w全部初始化为0
$$w=\begin{bmatrix}
  0 & 0 \\
  0 & 0
\end{bmatrix}$$
此时，对于各个输入样本，其对应的第一层参数$a^{[1]}_{1}=a^{[1]}_{2}$，就完全相等了。
该层两个节点一直在计算同样的函数，即权重为：$$
w = \begin{bmatrix}
  u & v \\
  u & v
\end{bmatrix}
$$
也就是无论经过多少次迭代计算后，权重每一层都是相同的。（对称性）

- Random initialization
	$w^{[1]} = np.random.randn((2,2)) * 0.01$
	$b^{[1]}=np.zero((2,1))$
	$w^{[2]}=np.random.randn((2,1)) * 0.01$
	$b^{[2]} = 0$
对$w^{[1]}$中，乘上的0.01这个系数，一般是为了让权重有一个较小的初始值。原因是因为网络使用的激活函数一般斜率较大的地方都在靠近0的附近。乘以一个较小的系数能保证网络开始迭代学习时，权重等参数能够有一个较大的变化步长。
*如果训练的网络是一个深层网络，可能需要更改0.01的数值*

