
### Vectorizing Logistic Regression's Gradient Computation

```python
# for-loops
J = 0, dW1 = 0, dW2 = 0, db = 0
for iter in range(1000): # 最外层的学习循环
	# 内部为单次的梯度下降迭代
	for i=1 to m:
		z(i) = w.T * x(i) + b
		a(i) = σ(z(i))
		J += - [y(i)log(a(i)) + (1-y(i))log(1-a(i))]
		dz(i) = a(i) - y(i)
		dw1 += x1(i)dz(i)
		dw2 += x2(i)dz(i)
		db += dz(i)
	J = J/m, dW1 = dW1/m, dW2=dW2/m, db=db/m

# 向量化
for iter in range(1000): # 最外层的学习循环
	# 内部为单次的梯度下降迭代
	Z = np.dot(w.T, X) + b
	A = sigmoid(Z)
	
	dZ = A - Y
	dw = 1/m * np.dot(X, dZ.T)
	db = 1/m * np.sum(dZ)
	
	w = w - a * dw
	b = b - a * db

```