## 统计学习方法三要素

$$方法=模型+策略+算法$$
### 模型

**统计学习的首要问题**：学习什么样的模型

- 监督学习中的模型：
	- 条件概率分布
	- 决策函数

假设空间 hypothesis space：定义该空间包含了所有可能的条件概率分布或决策函数。

- 数学表示：
	- 决策函数的集合：$$\mathcal{F}=\{f|Y=f(X)\}=\{f|Y=f_\theta(X),\theta \in \mathbf{R}^n\}$$
		- 其中，X 和 Y 是定义在输入空间 $\mathcal{X}$ 和输出空间 $\mathcal{Y}$ 上的变量；参数向量 $\theta$ 取值于 n 维欧氏空间 $\mathbf{R}^n$，**参数空间** parameter space
	- 条件概率的集合：$$\mathcal{F} = \{P|P(Y|X)\}=\{P|P_\theta(Y|X), \theta\in\mathcal{R}^n\}$$

- 由**决策函数**表示的模型为**非概率**模型
- 由**条件概率**表示的模型为**概率**模型

### 策略

前提：模型的假设空间确定。那么**统计学习的目的**就是：从假设空间中，选取最优模型

但是，
- 如何评估最优？
	- 所以要指定评估标准
	- 引入**损失函数（评估模型的单次预测）** 和 **风险函数（评估平均表现下模型的预测）** 的概念

#### 损失函数和风险函数

- 用以度量错误程度

常用的损失函数：
- 0-1损失函数，0-1 loss function：![[Pasted image 20231105170738.png]]
- 平方损失函数，quadratic loss function：![[Pasted image 20231105170808.png]]
- 绝对损失函数 absolute loss function：![[Pasted image 20231105170822.png]]
- 对数损失函数 logarithmic loss function / 对数似然损失函数 log-likelihood loss function：![[Pasted image 20231105170853.png]]

**风险函数 risk function 或 期望损失 expected loss**：理论上模型关于联合分布的平均意义下的损失。**损失函数的期望**：![[Pasted image 20231105170927.png]]

至此，**学习的目标**具体为，**选择期望风险最小的模型**。

问题：
- 联合分布P(X,Y)是未知的，R_{exp}(f)无法直接计算
- 病态问题 ill-formed problem
	- 根据期望风险最小学习模型，需要知道联合分布
	- 但联合分布是未知的

所以，这里给出一个给定训练数据集T={(x1,y1),...,(xN,yN)}。定义，**经验风险 empirical risk 或 经验损失 empirical loss**，是模型f(X)关于给定训练数据集的平均损失，记作 R_emp。![[Pasted image 20231105171700.png]]

- 根据大数定律，当样本容量N趋于无穷时，**经验风险**趋近于**期望风险**
- 所以，可以使用经验风险估计期望风险
- 但这样存在一些问题：
	- 现实的训练样本数目有限，这种估计并不理想
- 所以就需要对经验风险进行矫正

这种矫正方法是监督学习中的基本策略：
- **经验风险最小化**
- **结构风险最小化**

#### 经验风险最小化和结构风险最小化

这里我们假设，目标函数的假设空间、损失函数以及训练数据集已经确定。

**经验风险最小化 empirical risk minimization, ERM**：
- 此时，经验风险最小化就可以通过R_emp计算公式确定
- 按照这一策略，**认为R_emp最小的模型就是最优的模型**
- 即求解最优化问题：![[Pasted image 20231105172150.png]]
- 当样本容量足够大时，这种策略能够保证有很好的学习效果
- **极大似然估计 maximum likelihood estimation**即为经验风险最小化的一个例子。
	- 模型：条件概率分布、损失函数：对数损失函数；此时经验风险最小化=极大似然估计

**但如果样本的容量不足以达到最低限度的样本数量时？**
- 会出现**过拟合over-fitting**现象

为了防止过拟合，于是提出了下边的策略。

**结构风险最小化 structural risk minimization SRM**：
- SRM等价于正则化 regularization
- 结构风险 = 经验风险 + 正则化项 regularizer / 罚项 penalty term
- 于是有：![[Pasted image 20231105172857.png]]

- J(f)为模型的复杂度。模型f越复杂，复杂度J(f)就越大。
- λ则用以平衡经验风险和模型复杂度
- **贝叶斯估计中的最大后验概率估计 maximum posterior probability estimation, MAP**就是结构风险最小化的一个例子：模型，条件概率模型；损失函数，对数损失函数；模型复杂度由模型的先验概率表示时，二者等价
- 按照该策略，**结构风险最小的模型就是最优的模型**
- 即求最优化问题：![[Pasted image 20231105173217.png]]

### 算法

算法：学习模型的具体计算方法。

统计学习基于训练数据，根据学习策略，从假设空间中选取最优模型，最后考虑**用何种计算方法求解最优模型**。

在**策略**部分，将统计学习中选取最优模型问题转化为了，经验风险最小化或结构风险最小化对应的最优化问题。

所以，这里的算法就是**求解最优化问题**的算法

- 解析解一般不存在
- 用数值计算的方法求解

**求解最优化算法的问题：**
- 如何保证找到全局最优解
- 如何保证求解的过程非常高效

---

统计学习方法之间的不同，主要来自模型、策略、算法的不同。

确定了模型、策略、算法，统计学习的方法也就确定了。

所以，模型、策略和算法被称为统计学习的三要素。
