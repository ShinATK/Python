
虽然给出了模型的评估标准，但如何选择模型并不是观察测试误差或者训练误差哪一方更小。

总结表格：

| 误差现象                 | 现象定义 | 原因                                         | 解决方案 |
| ------------------------ | -------- | -------------------------------------------- | -------- |
| 训练误差、测试误差都很大 | 欠拟合   | 模型复杂度过低；特征量少；等等                     |  增加模型复杂度；增加数据特征；等等        |
| 训练误差小、测试误差大   | 过拟合   | 训练数据过少；复杂度过高；等等 | 增加训练数据；使用正则化约束；降低模型复杂度；等等         |

## 过拟合与模型选择

假设空间中存在具有不同复杂度的模型时，就面临模型选择 model selection问题。

一个需要遵循的标准，如果假设空间中存在“真”模型，那么我们所选择的模型应该是**逼近真模型**的。具体来说：
- 参数个数相同
- 参数向量与真参数向量相近

- 一味追求提高对训练数据的预测能力，所选模型的复杂度则往往比真模型更高。从而出现过拟合现象 over-fitting
- 这种情况下一般是学习时选择的模型参数过多，模型对已知数据预测得很好，但在未知数据上表现很差

**模型选择旨在避免过拟合并提高模型的预测能力**

![[Pasted image 20231105175106.png]]

需要确定模型选择的方法。

## 正则化与交叉验证

### 正则化 regularization

正则化：结构风险最小化的实现。![[Pasted image 20231105175933.png]]

- **正则化符合奥卡姆剃刀Occam's razor原理**：在所有可能选择的模型中，能够很好解释已知数据并且十分简单才是最好的模型，也就是应该选择的模型。
- 从贝叶斯估计的角度，正则化项对应于模型的先验概率

### 交叉验证 cross validation

样本数据充足的情况下，可以随机将数据集切成三部分：
- **训练集 training set：训练模型**
- **验证集 validation set：模型的选择**
- **测试集 test set：最终对学习方法的评估**

然而现实里很多实际应用中，数据是不充足的。

交叉验证的基本思想：重复地使用数据

1. 简单交叉验证
	- 随即将已知数据分为两部分，训练集和测试集（如：70%+30%）
	- 训练不同（如：不同参数个数）模型
	- 测试集上评估各个模型，选出测试误差最小的模型
2. S折交叉验证 S-flod cross validation
	- 随机将已知数据切分成S个**互不相交**、**大小相同**的子集
	- 利用S-1个子集的数据训练模型，余下的用于测试
	- 对这一过程可能的S种选择重复进行
	- 选出S次评测中平均测试误差最小的模型
3. 留一交叉验证 leave-one-out cross validation
	- S折交叉验证的特殊情况：S=N
	- 一般在数据缺乏的时候使用