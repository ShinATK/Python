# 第一章 统计学习及监督学习概论

## 1.1 统计学习

* **统计学习 statistical learning**：关于计算机基于数据构建概论统计模型并运用模型对数据进行预测与分析的一门学科
* 研究对象是**数据（data）**：从数据出发，提取数据特征，抽象出模型，发现数据中的知识，再回到对数据的分析与预测中去。**基本假设：同类数据具有一定的统计规律性**
* 目的：用于对数据，特别是**未知新数据的预测与分析**
* 方法：监督学习 supervised learning、无监督学习 unsupervised learning、强化学习 reinforcement learning
* **统计学习方法的三要素：模型 model、策略 strategy、算法 algorithm**

## 1.2 统计学习的分类

### 1.2.1 基本分类

1.  **监督学习 supervised learning**
    
    定义：从标注数据中学习预测模型的机器学习问题。
    
    **本质：学习输入到输出的映射的统计规律**
    
    1.  输入空间 input space、特征空间 feature space 和输出空间 output space
        
        训练集表示方法：$T=\{ (x_1,y_1),(x_2,y_2),…,(x_N,y_N) \}$
        
    2.  联合概率分布：假设输入输出的随机变量$X$和$Y$遵守联合概率分布$P(X,Y)$
    3.  **假设空间** **hypothesis space：**监督学习的模型可以是概率模型或非概率模型，由条件概率分布$P(Y|X)$或决策函数（decision function）$Y=f(x)$表示
    4.  问题的形式化：学习和预测两个过程

2.  无监督学习 unsupervised learning
    
    定义：从无标注数据中学习预测模型的机器学习问题。
    
    **本质：学习数据中的统计规律或潜在结构**
    
3.  强化学习 reinforcement learning
    
    定义：智能系统在与环境的连续互动中学习最优行为策略的机器学习问题。
    
    假设智能系统与环境的互动基于马尔可夫决策过程 Markov decision process，智能系统观测到的是与环境互动得到的数据序列。
    
    **本质：学习最优的序贯决策**
    
4.  半监督学习与主动学习

### 1.2.2 按模型分类

1.  **概率模型 probabilistic model 与非概率模型 non-probabilistic model或确定性模型 deterministic model**
    
    * 监督学习中
        * 概率模型/**生成模型**：条件概率分布形式$P(y|x)$
        * 非概率模型/**判别模型**：函数形式$y=f(x)$
    * 非监督学习中
        * 概率模型：条件概率分布形式$P(z|x)$或$P(x|z)$
        * 非概率模型：函数形式$z=g(x)$
    
	- *概率模型例子*：决策树、朴素贝叶斯、隐马尔可夫模型、条件随机场、概率潜在语义分析、潜在狄利克雷分配、高斯混合模型
	- *非概率模型例子*：感知机、支持向量机、k近邻、AdaBoost、k均值、潜在语义分析、神经网络
	- 逻辑斯谛回归二者都可看作
	 
2. **线性模型 linear model 与非线性模型 non-linear model**
	 
	- 线性模型：感知机、线性支持向量机、k近邻、k均值、潜在语义分析
	- 非线性模型：核函数支持向量机、AdaBoost、神经网络
	 
3. **参数化模型 parametric model 与 非参数化模型 non-parametric model**
	
	- 参数化模型：感知机、朴素贝叶斯、逻辑斯谛回归、k均值、高斯混合模型
	- 非参数化模型：决策树、支持向量机、AdaBoost、k近邻、潜在语义分析、概率潜在语义分析、潜在狄利克雷分配
	
### 1.2.3 按算法分类

- **在线学习 online learning**：每次接受一个样本，进行预测与学习模型，并不断重复该操作的机器学习
- **批量学习 batch learning**：一次接受所有数据，学习模型，之后进行预测


### Δ 1.2.4 按技巧分类

1. **贝叶斯学习 Bayesian learning**
	
	**贝叶斯学习 Bayesian learning / 贝叶斯推理 Bayesian inference**：在*概率模型*的学习和推理中，利用[[贝叶斯定理]]，计算*在给定数据条件下模型的条件概率*，即后验概率，并应用这个原理进行*模型估计*与对*数据预测*。
	
	例子：朴素贝叶斯、潜在狄利克雷分配
	
	记随机变量$D$表示数据，随机变量$\theta$表示模型参数，根据贝叶斯定理，后验概率$P(\theta|D)$为：$$P(\theta|D)=\frac{P(\theta)P(D|\theta)}{P(D)}$$
	其中，$P(\theta)$是先验概率，$P(D|\theta)$是后验概率
	
	*模型估计时，估计整个后验概率分布$P(\theta|D)$，如果需要给出一个模型，通常取后验概率最大的模型*
	
	后验概率分布的期望值：$$P(x|D)=\int P(x|\theta,D)P(\theta|D)\mathrm{d}\theta$$
2. 核方法
	
	**核方法 kernel method**：使用核函数表示和学习非线性模型的一种机器学习方法，可以用于监督学习和无监督学习。
	
	例子：核函数支持向量机、核PCA、核k均值


## 1.3 统计学习方法三要素

$$方法=模型+策略+算法$$

### 1.3.1 模型

确定**学习什么样的模型（所有可能的条件概率分布或决策函数）**

1. **决策函数的集合**：
	
	定义假设空间为决策函数的集合：$$\mathcal{F}=\{ f|Y=f(X) \}$$
	则，$\mathcal{F}$为由一个参数向量决定的函数族：$$\mathcal{F}=\{ f|Y=f_\theta(X) \}$$

2. **条件概率的集合**：
	
	定义假设空间为条件概率的集合：$$\mathcal{F}=\{ P|P(Y|X) \}$$
	则，$\mathcal{F}$为由一个参数向量决定的条件概率分布族：$$\mathcal{F}=\{ P|P_\theta(Y|X), \theta \in \mathbf{R}^n \}$$
	

### 1.3.2 策略

确定**按照什么样的准则学习或（从假设空间中）选择最优的模型**

1. **损失函数** 和 **风险函数**
	
	常用的损失函数有：
	
	（1）0-1损失函数（**0-1 loss function**）$$L(Y,f(X))=\left\{\begin{matrix} 1, Y \ne f(X) \\ 0, Y = f(X) \end{matrix}\right.$$
	（2）平方损失函数（**quadratic loss function**）$$L(Y,f(X))=(Y-f(X))^2$$
	（3）绝对损失函数（**absolute loss function**）$$L(Y, f(X)) = |Y-f(X)|$$
	（4）对数损失函数（**logarithmic loss function**）$$L(Y,P(Y|X))=-\log P(Y|X)$$
	**损失函数的期望**（损失函数$L(Y,f(X))$的期望值）：$$R_{exp}(f) = E_p [L(Y,f(X))] = \int_{\mathcal{X} \times \mathcal{Y}}L(y,f(x))P(x,y)\mathrm{d}x\mathrm{d}y$$
	这是理论上模型$f(X)$关于联合分布$P(X,Y)$的平均意义下的损失，称为**风险函数(risk function)** 或者 **期望损失(expected loss)**
	
	*但联合分布$P(X,Y)$是未知的*
	
	**经验风险(empirical risk)**：模型$f(X)$关于训练数据集的平均损失（损失函数$L(Y,f(X))$的均值）$$R_{emp}(f)=\frac{1}{N}\sum\limits^N_{i=1} L(y_i,f(x_i))$$
	当样本容量$N$趋近于无穷时，经验风险$R_{emp}(f)$趋于期望风险$R_{exp}(f)$
	
2. **经验风险最小化** 和 **结构风险最小化**
	
	用经验风险去估计期望风险常常不理想，所以需要对经验风险进行矫正。
	
	- **经验风险最小化(empirical risk minimization, ERM)**：
		经验风险最小的模型就是最优的模型，即：$$\min\limits_{f \in \mathcal{F}}\frac{1}{N}\sum\limits^N_{i=1}L(y_i, f(x_i))$$
		例子：**极大似然估计（maximum likelihood estimation, MLE）** 就是经验风险最小化的例子。*当模型是条件概率分布、损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计*。
		
		缺点：样本容量很小时，会产生“**过拟合现象（over-fitting）**”
		
	- **结构风险最小化(structural risk minimization, SRM)**：
		防止过拟合而提出的策略，等价于正则化（regularization）。
		
		在假设空间、损失函数以及训练数据集确定的情况下，结构风险定义为：$$R_{srm}(f)\frac{1}{N}\sum\limits^N_{i=1}L(y_i,f(x_i))+\lambda J(f)$$
		例子：贝叶斯估计中的**最大后验概率估计（maximum posterior probability estimation, MAP）** 就是结构风险最小化的一个例子。*当模型是条件概率分布、损失函数是对数损失函数、**模型复杂度由模型的先验概率表示**时，结构风险最小化就等价于最大后验概率估计*
		
		结构风险最小的模型是最优的模型，即：$$\min\limits_{f\in \mathcal{F}}\frac{1}{N}\sum\limits^N_{f\in \mathcal{F}}L(y_i, f(x_i))+\lambda J(f)$$
### 1.3.3 算法

算法：用以求解最优模型的计算方法。

此时统计学习问题归结为最优化问题：
- 有解析解
- 无解析解：
	- 利用数值计算找到全局最优解，*并使求解的过程非常搞笑*


## 1.4 模型评估与模型选择
### 1.4.1 训练误差与测试误差

统计学习的目的：*使学到的模型不仅对已知数据而且对未知数据都能有很好的预测能力*

当损失函数确定时，基于损失函数的模型的**训练误差（training error）**$$R_{emp}(\hat{f})=\frac{1}{N}\sum\limits^N_{i=1}L(y_i,\hat{f}(x_i))$$
和 **模型的测试误差（test error）** $$e_{test}=\frac{1}{N'}\sum\limits^{N'}_{i=1}L(y_i,\hat{f}(x_i))$$作为学习方法评估的标准。

### 1.4.2 过拟合与模型选择

假设空间中存在有不同复杂度的模型时，就需要解决模型选择（model selection）的问题

如果在假设空间中存在“真”模型，那么所选择的模型应该逼近真模型（模型参数个数应该相同，参数向量应与真参数向量相近）

只是提高对训练数据的预测能力，所选模型的复杂度可能比真模型更高，从而导致**过拟合 over-fitting**。

**过拟合**：学习时选择的模型所包含的参数过多，以至于出现这一模型对已知数据预测的很好，但对未知数据预测得很差的现象。


## 1.5 正则化与交叉验证

### 1.5.1 正则化

**正则化**是*结构风险最小化策略的实现*，在经验风险的基础上加一个*正则化项（regularizer）或罚项（penalty term）*

形式一般为：$$\min\limits_{f \in \mathcal{F}}\frac{1}{N}\sum\limits^N_{i=1}L(y_i,f(x_i))+\lambda J(f)$$
其中，$\lambda \ge 0$为调整经验风险项与正则化项之间关系的系数

### 1.5.2 交叉验证

**交叉验证 cross validation**：基本想法，重复地使用数据；把给定的数据进行切分，将切分的数据集组合为训练集与测试机，在此基础上反复训练、测试以及模型选择。

1. 简单交叉验证
	- 随机将已知数据分为两部分，一部分作为训练集，一部分作为测试机；
	- 用训练集在各种条件下（如：不同参数个数）训练模型，从而得到不同的模型；
	- 在测试集上评价各个模型的测试误差，选出测试误差最小的模型。
1. **S折交叉验证（S-fold cross validation）**
	- 随机将已知数据切分成$S$个互不相交、大小相同的子集（每个子集里有N/S个数据）
	- 利用$S-1$个子集的数据训练模型，使用余下的子集测试模型
	- *将这一过程对可能的$S$种选择重复进行*
	- 选出$S$次评测中平均测试误差最小的模型
2. 留一交叉验证 leave-one-out cross validation
	- S折交叉验证的特殊情形：$S=N$（每个子集里有1个数据）


## 1.6 泛化能力

### 1.6.1 泛化误差

**泛化能力（generalization ability）**：由该方法学习到的模型对未知数据的预测能力，是学习方法本质上重要的性质。

泛化误差的定义：如果学到的模型是$\hat{f}$，那么用这个模型对未知数据预测的误差即为泛化误差（generalization error）：$$R_{exp}(\hat{f})=E_P[L(Y,\hat{f}(X))]=\int_{\mathcal{X}\times\mathcal{Y}}L(y,\hat{f}(x))P(x,y)\mathrm{d}x\mathrm{d}y$$

注：这里的模型是已经经过学习后得到的模型$\hat{f}$

*事实上，泛化误差就是**所学习到的模型**的期望风险*

### 1.6.2 泛化误差上界

学习方法的泛化能力分析往往是通过研究泛化误差的概率上界进行的，简称**泛化误差上界（generalization error bound）**。

泛化误差上界的性质：

- 是样本容量的函数，样本容量增加时，泛化上界趋于0；
- 是假设空间容量的函数，假设空间越大，模型就越难学，泛化误差上界就越大

**定理 1.1 （泛化误差上界）**：对二分类问题，当假设空间是有限个函数的集合$\mathcal{F}=\{ f_1,f_2,...,f_d \}$时，对任意一个函数$f \in \mathcal{F}$，至少以概率$1-\delta, 0 < \delta < 1$，以下不等式成立：$$R(f) \leq \hat{R}(f) + \varepsilon(d,N,\delta)$$
其中，$R(f)$是泛化误差，右侧为泛化误差上界，$\hat{R}(f)$为训练误差，$\varepsilon(d,N,\delta)=\sqrt{\frac{1}{2N}(\log d + \log \frac{1}{\delta})}$， $\hat{R}(f)$为训练误差、
[[泛化误差上界 定理1.1 证明]]

## 1.7 生成模型与判别模型

**监督学习方法**：
- *生成方法（generative approach）*：
	由数据学习联合概率分布$P(X,Y)$，然后求出条件概率分布$P(Y|X)$作为预测的模型，即*生成模型 generative model*：$$P(Y|X)=\frac{P(X,Y)}{P(X)}$$
	模型表示了给定输入$X$产生输出$Y$的*生成关系*。
	
	朴素贝叶斯法和隐马尔可夫模型
	
	**特点：**
	- 可以*还原出联合概率分布*$P(X,Y)$；
	- 学习*收敛速度很快*，样本容量增加的时候，学到的模型可以更快地收敛于真实模型
	- *存在隐变量时，仍可以用生成方法学习*
	
- *判别方法（discriminative approach）*：
	由数据直接学习决策函数$f(X)$或者条件概率分布$P(Y|X)$作为预测的模型，即*判别模型 discriminative model*
	
	对给定的输入$X$，应该预测什么样的输出$Y$
	
	k近邻法、感知机、决策树、逻辑斯谛回归模型、最大熵模型、支持向量机、提升方法和条件随机场等
	
	**特点：**
	- 直接学习的是决策函数$f(X)$或者条件概率分布$P(Y|X)$，直接面对预测，*学习的准确率更高*
	- 直接学习决策函数$f(X)$或者条件概率分布$P(Y|X)$，可以对数据进行各种程度上的抽象、定义特征并使用特征，*因此可以简化学习问题*


## 1.8 监督学习应用

1. 分类问题
2. 标注问题
3. 回归问题

### 1.8.1 分类问题

在监督学习中，*当输出变量$Y$取有限个离散值时*，预测问题便成为分类问题。

- 输入$X$：离散或连续变量
- 输出$Y$：有限个离散值

评价分类器性能标准：*分类准确率（accuracy）*，定义为，对于给定的测试数据集，分类器正确分类的样本数与总样本数之比。即损失函数是0-1损失时测试数据集上的准确率/

**二分类问题中**：常用的评价指标为*精确率（precision）* 与 *召回率（recall）*。

| 实际\预测 | 正类预测 | 负类预测 |
| --------- | ---- | ---- |
| 正类输入      |  TP    |   FN   |
| 负类输入          |  FP    |  TN   |

**精确率**定义为：$$P=\frac{TP}{TP+FP}$$

**召回率**定义为：$$R=\frac{TP}{TP+FN}$$

$F_1$值：精确率与召回率的调和均值：$$\frac{2}{F_1}=\frac{1}{P}+ \frac{1}{R}$$

可以用于分类的统计学习方法：k近邻法、感知机、朴素贝叶斯法、决策树、决策列表、逻辑斯谛回归模型、支持向量机、提升方法、贝叶斯网络、神经网络、Winnow等。


### 1.8.2 标注问题

**标注（tagging）**，是分类问题的一个推广，更为复杂的结构预测（structure prediction）问题的简单形式。

目标是：学习一个能够*对观测序列给出标记序列作为预测*的模型

- 输入$X$：*观测序列*
- 输出$Y$：*标记序列* 或 *状态序列*

常用的学习方法有：隐马尔可夫模型、条件随机场


### 1.8.3 回归问题

**回归（regression）**：用于预测输入变量（自变量）和输出变量（因变量）之间的关系，特别是当输入变量发生变化时，输出变量随之发生的变化。

- 输入变量个数：一元回归和多元回归
- 输入变量和输出变量的关系：线性回归和非线性回归

常用的损失函数：平方损失函数。

---

# 本章概要

1. 统计学习或机器学习是关于计算机基于数据构建概率统计模型并运用模型对数据进行分析与预测的一门学科。统计学习包括：监督学习、无监督学习和强化学习
2. 统计学习方法三要素：**模型、策略、算法**。
3. 监督学习：从给定有限的训练数据出发，*假设数据是独立同分布的*，而且*假设模型属于某个假设空间*（**模型**），应用某一评价准则（**策略**），从假设空间中选取一个最优的模型（**算法**），使*它对已给训练数据及未知测试数据在给定评价标准意义下有最准确的预测*。
4. 进行模型选择或者说提高学习的泛化能力是一个重要问题。如果只考虑减少训练误差，就可能产生过拟合现象。模型选择的方法有正则化与交叉验证。学习方法泛化能力的分析是统计学习理论研究的重要课题。
5. 分类问题、标注问题和回归问题都是监督学习的重要问题。

---

# 习题

> 1.1 说明伯努利模型的极大似然估计以及贝叶斯估计中的统计学习方法三要素。伯努利模型是定义在取值0与1的随机变量上的概率分布。假设观测到伯努利模型$n$次独立的数据生成结果，其中$k$次的结果为$1$，这时可以用极大似然估计或贝叶斯估计来估计结果为$1$的概率.

[[第1章 习题 1.1]]

> 1.2 通过经验风险最小化推导极大似然估计。证明模型是条件概率分布，当损失函数是对数损失函数时，经验风险最小化等价于极大似然估计。

[[第1章 习题 1.2]]