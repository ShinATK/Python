> 1.1 说明伯努利模型的极大似然估计以及贝叶斯估计中的统计学习方法三要素。伯努利模型是定义在取值0与1的随机变量上的概率分布。假设观测到伯努利模型$n$次独立的数据生成结果，其中$k$次的结果为$1$，这时可以用极大似然估计或贝叶斯估计来估计结果为$1$的概率.

**解答：**

**解答思路**：
	1. 写出伯努利模型；
	2. 写出伯努利模型的极大似然估计以及贝叶斯估计中的统计学习方法三要素；
	3. 根据伯努利模型的极大似然估计，估计结果为1的概率；
	4. 根据伯努利模型的贝叶斯估计，估计结果为1的概率

**解答步骤：**

**第一步：伯努利模型**

根据题意：伯努利模型是定义在取值为0与1的随机变量上的概率分布。
对于随机变量$X$，则有：$$\begin{align} P(X=1) &= p \\ P(X=0) &= 1-p\end{align}$$
故伯努利模型可以写为：$$P_p(X=x)=p^x(1-p)^{1-x}, 0 \le p \le 1$$
从而得到伯努利模型的假设空间为：$$\mathcal{F}=\{ P|P_p(X)=p^x(1-p)^{1-x},p \in [0,1] \}$$

**第二步：伯努利模型的极大似然估计以及贝叶斯估计中的统计学习方法三要素**

（1）极大似然估计
	模型：伯努利模型
	策略：经验风险最小化。极大似然估计，等价于当模型是条件概率分布、损失函数是对数损失函数时的经验风险最小化。
	算法：极大似然估计：$\arg \max\limits_p L(p|X)=\arg \max\limits_p P(X|p)$
（2）贝叶斯估计
	模型：伯努利模型
	策略：结构风险最小化。贝叶斯估计中的最大后验概率估计，等价于当模型为条件概率模型、损失函数为对数损失函数，且模型复杂度由模型的先验概率表示时的结构风险最小化。
	算法：最大化后验概率：$\arg \max\limits_p \pi(p|X) = \arg \max\limits_p \frac{P(X|p)\pi(p)}{\int P(X|p)\pi(p)\mathrm{d}p}$

**第三步：伯努利模型的极大似然估计**

[# 极大似然估计的一般步骤 参考Wiki](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)

1. 写出随机变量的概率分布函数$$P_p(X=x)=p^x(1-p)^{1-x}, 0 \le p \le 1$$
2. 写出似然函数$$L(p|X)=P(X|p)=\prod\limits^n_{i=1}P(x^{(i)}|p)=p^k(1-p)^{n-k}$$
3. 对似然函数取对数，得到对数似然函数，并进行化简$$\begin{align} \hat{p} &=\arg \max\limits_p L(p|X) \\ \log L(p|X) &=\log {p^k(1-p)^{n-k}}=k \log p + (n-k)\log (1-p) \end{align}$$
4. 对参数进行求导，并令导数等于0$$\begin{align} \frac{\partial \log L(p)}{\partial p} &= \frac{k}{p} - \frac{{n-k}}{{1-p}} \\ &= \frac{{k-np}}{{p(1-p)}} \\ &= 0\end{align}$$
5. 求解似然函数方程，得到参数的值$$\begin{align} &p=\frac{k}{n} \\ &P(X=1)=\frac{k}{n} \end{align}$$

**第四步：伯努利模型的贝叶斯估计**

- 解法一：求最大后验估计
	
	[# 贝叶斯估计（最大后验估计）的一般步骤 参考Wiki](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation)
	
	1. 确定参数$\theta$的先验概率$p(\theta)$
		伯努利分布的先验分布为[# Beta分布](https://zh.wikipedia.org/wiki/%CE%92%E5%88%86%E5%B8%83)，则此处假设先验分布$\pi(p)$为Beta分布，所以$p$的先验分布为：$$\pi(p)=\frac{1}{{B(\alpha,\beta)}} p^{(\alpha-1)}(1-p)^{\beta-1}$$
	2. 根据样本集$D=\{ x_1, x_2, ... , x_n \}$，计算似然函数$P(D|\theta)$：$P(D|\theta)=\prod \limits^n_{i=1}P(x_i|D)$
		似然函数为：$$\begin{align} L(p|X) &= P(X|p) \\ &= \prod\limits^n_{i=1} P(x^{(i)}|p) \\ &= p^k(1-p)^{n-k} \end{align}$$
	3. 利用贝叶斯公式，写出后验概率最大化公式：$$\arg \max\limits_\theta P(\theta|D)=\arg \max\limits_\theta \frac{{P(D|\theta)P(\theta)}}{{\int_\Theta P(D|\theta)P(\theta)\mathrm{d}\theta}}=\arg\max\limits_\theta P(D|\theta)P(\theta)$$
		最大化后验概率，求解参数$p$：$$\begin{align} \hat{p} &= \arg \max\limits_p \frac{{P(X|p)\pi(p)}}{{\int P(X|p)\pi(p)\mathrm{d}p}}\\ &= \arg \max\limits_p P(X|p)\pi(p) \\ &= \arg \max\limits_p p^k(1-p)^{n-k} \frac{1}{{B(\alpha,\beta)}} p^{(\alpha-1)}(1-p)^{\beta-1} \\ &= \arg \max\limits_p \frac{1}{{B(\alpha,\beta)}} p^{(k+\alpha-1)}(1-p)^{n-k+\beta-1} \end{align}$$
		令$g(p) = \frac{1}{{B(\alpha,\beta)}} p^{(k+\alpha-1)}(1-p)^{n-k+\beta-1}$，对函数$g(p)$取对数，再对$p$求导，得$$\frac{\partial\log g(p)}{\partial p} = \frac{1}{{B(\alpha, \beta)}} (\frac{{k+\alpha-1}}{{p}}-\frac{n-k+\beta-1}{1-p})$$
	4. 利用求导，得到后验概率最大时的参数取值$$P(X=1)=\frac{k+\alpha-1}{n+\alpha+\beta-2}$$
- 解法二：求后验概率分布的期望
	
	[# 后验概率的期望求解 参考Wiki](https://en.wikipedia.org/wiki/Bayes_estimator)
	
	贝叶斯估计中的最大后验概率估计，得到的是模型参数$\theta$这个随机变量的后验分布的众数，通常被认为是点估计。而贝叶斯方法的特点是使用分布来总结数据和得出推论，因此贝叶斯方法倾向于得到后验均值或中值，以及可信区间。
	
	贝叶斯估计，利用后验分布的期望（均值）作为参数的估计值的方法，前两部与最大后验概率估计相同。
	1. 确定参数$\theta$的先验概率$p(\theta)$
	2. 根据样本集$D=\{ x_1, x_2, ... , x_n \}$，计算似然函数$P(D|\theta)$：$P(D|\theta)=\prod \limits^n_{i=1}P(x_i|D)$
	3. 利用贝叶斯公式，求$\theta$的后验概率：$$P(\theta|D)=\frac{P(D|\theta)P(\theta)}{\int\limits_\Theta P(D|\theta)P(\theta)\mathrm{d}\theta}$$
		参数$p$的后验分布为：$$\begin{align} P(p|X) &= \frac{P(X|p)\pi(p)}{\int P(X|p)\pi(p)\mathrm{d}p} \\ &= \frac{ \frac{1}{B(\alpha,\beta)}p^{k+\alpha-1}(1-p)^{n-k+\beta-1} }{ \int \frac{1}{B(\alpha,\beta)}p^{k+\alpha-1}(1-p)^{n-k+\beta-1}\mathrm{d}p } \\ &= \frac{p^{k+\alpha-1}(1-p)^{n-k+\beta-1}}{\int p^{k+\alpha-1}(1-p)^{n-k+\beta-1} \mathrm{d}p} \\ &= \frac{1}{B(k+\alpha,n-k+\beta)}p^{k+\alpha-1}(1-p)^{n-k+\beta-1} \\ &\sim Be(k+\alpha,n-k+\beta)\end{align}$$
	4. 计算后验概率分布参数$\theta$的期望，并求出贝叶斯估计值$\hat{\theta}=\int\limits_\Theta \theta \cdot P(\theta|D)\mathrm{d}\theta$
		后验概率期望为：$$\begin{align} E_p(p|X) &= E_p(Be(k+\alpha,n-k+\beta)) \\ &= \frac{k+\alpha}{(k+\alpha)+(n-k+\beta)} \\ &= \frac{k+\alpha}{n+\alpha+\beta} \end{align}$$则以参数的后验概率分布的期望作为贝叶斯估计的参数值：$$\hat{p}=\frac{k+\alpha}{n+\alpha+\beta}$$