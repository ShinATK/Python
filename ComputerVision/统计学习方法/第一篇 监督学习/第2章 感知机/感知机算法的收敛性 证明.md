#### 感知机算法的收敛性

#### 证明

（1）由于训练数据集是线性可分的，则存在超平面可将训练数据集完全正确分开，取此超平面为$\hat{w}_{opt} \cdot \hat{x} = w_{opt} \cdot x + b_{opt} = 0$，使$||\hat{w}_{opt}||=1$。对于有限的$i=1,2,...,N$均有$$y_i(\hat{w}_{opt} \cdot \hat{x}_i) = y_i(w_{opt} \cdot x_i + b_{opt}) > 0$$所以存在$$\gamma = \min\limits_i\{ y_i(w_{opt \cdot x_i} + b_{opt}) \}$$使$$y_i(\hat{w}_{opt}\cdot \hat{x}_i) = y_i(w_{opt} \cdot x_i +b_{opt}) \geq \gamma$$
（2）感知机算法从$\hat{w}_0 = 0$开始，如果实例被误分类，则更新权重。令$\hat{w}_{k-1}$使第$k$个误分类实例之前的扩充权重向量，即$$\hat{w}_{k-1}=(w^T_{k-1},b_{k-1})^T$$则第$k$个误分类实例的条件是$$y_i(\hat{w}_{k-1} \cdot \hat{x}_i) \leq 0$$
若$(x_i,y_i)$是被$\hat{w}_{k-1}=(w^T_{k-1},b^T_{k-1})^T$误分类的数据，则$w$和$b$的更新是$$\begin{align} w_k &\leftarrow w_{k-1} + \eta y_i x_i \\ b_k &\leftarrow b_{k-1} + \eta y_i \end{align}$$即$$\hat{w}_k = \hat{w}_{k-1} + \eta y_i \hat{x}_i$$

从而根据**证明公式1**和**证明公式2**可得：
$$\begin{align}

k\eta \gamma &\leq \hat{w}_k \cdot \hat{w}_{opt} \leq ||\hat{w}_k|| \ ||\hat{w}_{opt}|| \leq \sqrt{k}\eta R \\
k^2\gamma^2 &\leq kR^2

\end{align}$$

所以最后可以得到**收敛性证明**：$$k \leq (\frac{R}{\gamma})^2$$


#### 证明公式1

$$\hat{w}_k \cdot \hat{w}_{opt} \geq k \eta \gamma$$
**证明**：

$$\begin{align} \hat{w}_k \cdot \hat{w}_{opt} &= \hat{w}_{k-1} \cdot \hat{w}_{opt} + \eta y_i \hat{w}_{opt} \cdot \hat{x}_i \\ &\geq \hat{w}_{k-1} \cdot \hat{w}_{opt} + \eta \gamma \end{align}$$
由此进行递推可得：$$\begin{align} \hat{w}_k \cdot \hat{w}_{opt} &\geq \hat{w}_{k-1} \cdot \hat{w}_{opt} + \eta \gamma \\ &\geq \hat{w}_{k-2} \cdot \hat{w}_{opt} + 2 \eta \gamma \\ &... \\ &\geq k\eta \gamma \end{align}$$

#### 证明公式2
$$||\hat{w}_k||^2 \leq k\eta^2 R^2$$

**证明**：

$$\begin{align}
||\hat{w}_k||^2 &= ||\hat{w}_{k-1}||^2 + 2\eta y_i \hat{w}_{k-1} \cdot \hat{x}_i + \eta^2||\hat{x}_i||^2 \\
&\leq ||\hat{w}_{k-1}||^2 + \eta^2 ||\hat{x}_i||^2 \\
&\leq ||\hat{w}_{k-1}||^2 + \eta^2 R^2 \\
&\leq ||\hat{w}_{k-2}||^2 + 2 \eta^2 R^2 \leq ... \\
&\leq k\eta^2 R^2
\end{align}$$