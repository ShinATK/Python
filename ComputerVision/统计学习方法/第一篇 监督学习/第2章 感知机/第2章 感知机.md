
# [第二章 感知机](zotero://open-pdf/0_9ILI29JS/55)

Perceptron 二分类的线性分类模型，输入为实例的特征向量，输出为实例的类别，取+1和-1二值。

判别模型

**旨在求出将训练数据进行线性划分的分离超平面**

## 2.1 [感知机模型](zotero://open-pdf/0_9ILI29JS/55)

**定义2.1（感知机）：** 假设输入空间（特征空间）是$\mathcal{X} \in \mathbf{R}^n$，输出空间是$\mathcal{Y} = \{ +1, -1 \}$。输入$x \in \mathcal{X}$表示实例的特征向量，对应于输入空间（特征空间）的点；输出$y \in \mathcal{Y}$表示实例的类别。由输入空间到输出空间的如下函数：$$f(x) = \mathrm{sign} (w \cdot x + b)$$称为感知机，其中，$w$和$b$为感知机模型参数，$w \in \mathbf{R}^n$叫做权值（weight）或权值向量（weight vector），$b \in \mathbf{R}$叫做偏执（bias），$w \cdot x$表示$w$和$x$的内积。$\mathrm{sign}$是符号函数，即$$\mathrm{sign} = \left\{\begin{matrix} +1, x\geq 0 \\ -1,x < 0 \end{matrix}\right. $$

*感知机是一种线性分类模型*

**感知机的几何解释：**
	线性方程$$w \cdot x + b = 0$$
	对应于特征空间$\mathbf{R}^n$中的一个超平面$S$，其中$w$是超平面的法向量，$b$是超平面的截距。这个超平面将特征空间划分为两个部分。位于两部分的点（特征向量）分别被分为正、负两类。因此，超平面$S$被称为**分离超平面（separating hyperplane）**。
	![[Pasted image 20230808113126.png]]


## 2.2 [感知机学习策略](zotero://open-pdf/0_9ILI29JS/56)

### 2.2.1 数据集的线性可分性

**定义2.2（数据集的线性可分性）：** 给定一个数据集$$T=\{ (x_1,y_1),(x_2,y_2),...,(x_N,y_N) \}$$其中，$x_i \in \mathcal{X} = \mathbf{R}^n,y_i \in \mathcal{Y} = \{ +1,-1 \},i=1,2,...,N$，如果存在某个超平面$S$ $$w \cdot x + b = 0$$能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有$y_i = +1$的实例$i$，有$w \cdot x_i + b > 0$，对所有$y_i = -1$的实例$i$，有$w \cdot x_i + b < 0$，则称数据集$T$为线性可分数据集（linearly separable data set）；否则，称数据集$T$线性不可分。

### 2.2.2 [感知机学习策略](zotero://open-pdf/0_9ILI29JS/57)

**感知机的学习目标：** 
	求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面。

**损失函数的选择：** 
	误分类点到超平面$S$的总距离。

输入空间$\mathbf{R}^n$中任一点$x_0$到超平面$S$的距离：$$\frac{1}{||w||} |w\cdot x_0 + b|$$
这里，$||w||$是$w$的$L_2$范数。

对于误分类的数据$(x_i,y_i)$来说，$$-y_i(w\cdot x_i + b)>0$$成立。

所以，
	给定训练数据集$T=\{ (x_1,y_1),(x_2,y_2),...,(x_N,y_N) \}$ 其中，$x_i \in \mathcal{R}^n,y_i\in\mathcal{Y}=\{ +1,-1 \},i=1,2,...,N$。感知机$\mathrm{sign}(w\cdot x + b)$学习的损失函数定义为$$L(w,b)=-\sum\limits_{x_i\in M}y_i(w\cdot x_i+b)$$
	其中，$M$为误分类点的集合。这个损失函数就是感知机学习的**经验风险函数**。


## 2.3 [感知机学习算法](zotero://open-pdf/0_9ILI29JS/58)

### 2.3.1 感知机学习算法的原始形式

**感知机学习算法**：
	给定一个训练数据集$$T=\{ (x_1,y_1),(x_2,y_2),...,(x_N,y_N) \}$$其中，$x_i \in \mathcal{R}^n,y_i\in\mathcal{Y}=\{ +1,-1 \},i=1,2,...,N$，求参数$w, b$，使其为一下损失函数极小化问题的解：$$\min\limits_{w,b}L(w,b) = - \sum\limits_{x_i \in M}y_i(w \cdot x_i + b)$$其中$M$为误分类点的集合。

误分类驱动，具体采用随机梯度下降法（stochastic gradient）

**算法2.1 （感知机学习算法的原始形式）**
	输入：训练数据集$T=\{ (x_1,y_1),(x_2,y_2),...,(x_N,y_N) \}$，其中$x_i \in \mathcal{X} = \mathbf{R}^n, y_i \in \mathcal{Y} = \{ +1,-1 \},i=1,2,...,N$；学习率$\eta(0<\eta \leq 1)$；
	输出：$w, b$；感知机模型$\mathrm{sign}(w\cdot x + b)$
	（1）选取初值$w_0,b_0$；
	（2）在训练集中选取数据$(x_i,y_i)$；
	（3）如果$y_i(w\cdot x_i + b) \leq 0$，
	$$\begin{align} \bigtriangledown_w L(w,b) &= -\sum\limits_{x_i \in M}y_i x_i \qquad w \leftarrow w + \eta y_i x_i \\ \bigtriangledown_b L(w,b) &= -\sum\limits_{x_i \in M}y_i \qquad b \leftarrow b + \eta y_i \end{align}$$
	（4）转至（2），直至训练集中没有误分类点


### 2.3.2 算法的收敛性

**定理2.1（Novikoff）** 设训练数据集$T=\{ (x_1,y_1),(x_2,y_2),...,(x_N,y_N) \}$是线性可分的，其中$x_i \in \mathcal{X}=\mathbf{R}^n,y_i \in \mathcal{Y}=\{ -1,+1 \},i=1,2,...,N$则
	（1）存在满足条件$||\hat{w}_{opt}||=1$的超平面$\hat{w}_{opt} \cdot \hat{x} = w_{opt} \cdot x + b_{opt} = 0$将训练数据集完全正确分开；且存在$\gamma > 0$，对所有$i=1,2,...,N$都有$$y_i(\hat{w}_{opt} \cdot \hat{x}_i) \geq \gamma$$
	（2）令$R=\max\limits_{1 \leq i \leq N} ||\hat{x}_i||$，则感知机算法2.1在训练集上的误分类次数$k$满足不等式$$k \leq (\frac{R}{\gamma})^2$$
**[[感知机算法的收敛性 证明]]**

*定理表明，误分类的次数$k$是有上界的，经过有限次搜索可以找到将训练数据完全正确分开的分离超平面。*
即：
	当训练数据集线性可分时，感知机学习算法原始形式迭代是收敛的。

注：感知机学学习算法存在许多解，这些解既依赖于初值的选择，也依赖于迭代过程中误分类点的选择顺序。为了得到唯一的超平面，需要对分离超平面增加约束条件。


### 2.3.3 感知机学习算法的对偶形式

*对偶形式的基本想法*：将$w$和$b$表示为实例$x_i$和标记$y_i$的线性组合的形式，通过求解其系数而求得$w$和$b$。

假设初始值$w_0,b_0$均为0，利用误分类点$(x_i,y_i)$对$w,b$进行修改$n$次，则最后学习到的$w,b$可以表示为：$$\begin{align} w &= \sum\limits^N_{i=1}\alpha_i y_i x_i \\ b &= \sum\limits^N_{i=1} \alpha_i y_i \end{align}$$这里，$\alpha_i = n_i \eta_i \geq 0, i=1,2,...,N$。实例点更新次数越多，意味着它距离分离超平面越近，也就越难正确分类。

**算法2.2 （感知机学习算法的对偶形式）**：
	输入：线性可分的数据集$T=\{ (x_1,y_1),(x_2,y_2),...,(x_N,y_N) \}$，其中$x_i \in \mathbf{R}^n, y_i \in \{ -1,+1 \},i=1,2,...,N$；学习率$\eta (0<\eta \leq 1)$
	输出：$\alpha, \beta$；感知机模型$f(x) = \mathrm{sign}(\sum\limits^N_{j=1}\alpha_i y_i x_i \cdot x + b)$，其中$\alpha = (\alpha_1,\alpha_2,...,\alpha_N)^T$
	（1）$\alpha \leftarrow 0, b \leftarrow 0$；
	（2）在训练集中选取数据$(x_i,y_i)$；
	（3）如果$y_i( \sum\limits^N_{j=1} \alpha_j y_j x_j \cdot x_i + b) \leq 0$，$$\begin{align} \alpha_i \leftarrow \alpha_i + \eta \\ b \leftarrow b + \eta y_i \end{align}$$
	（4）转至（2）直到没有误分类数据。

