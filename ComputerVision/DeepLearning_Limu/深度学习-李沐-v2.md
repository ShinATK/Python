# 3.线性神经网络
## 3.1.线性回归
**回归-regression**：
	为*一个或多个自变量*与*因变量*之间关系建模的一类方法。
### 3.1.1.线性回归的基本元素

### 3.1.4.随机梯度下降
梯度下降-gradient descent
通过不断在损失函数递减的方向上更新参数来降低误差

## 3.2.线性回归的从零开始实现
1. 数据流水线
2. 模型
3. 损失函数
4. 小批量随机梯度下降优化器

## 总结
训练softmax回归循环模型与训练回归模型非常相似：
1. 读取数据
2. 定义模型和损失函数
3. 使用优化算法训练模型
4. 使用测试数据进行测试

---
# 5.深度学习计算
## 5.1 层和块

事实证明，研究讨论“比单个层大”但“比整个模型小”的组件更有价值

在计算机视觉中广泛流行的**ResNet-152**架构就有数百层， 这些层是由*层组*（groups of layers）的重复模式组成。 这个ResNet架构赢得了2015年ImageNet和COCO计算机视觉比赛 的识别和检测任务 ([He _et al._, 2016](https://ieeexplore.ieee.org/document/7780459))。 目前ResNet架构仍然是许多视觉任务的首选架构。 在其他的领域，如自然语言处理和语音， 层组以各种重复模式排列的类似架构现在也是普遍存在。

- **神经网络块**，块 block，对由多个层组成的组件进行的抽象
	- 块由类表示
	- 任何子类须有前向传播函数，并可以存储任何必须的参数
	- 必须有反向传播函数（自动微分提供了后端实现）

```python
net = nn.Sequential(
		nn.Linear(20, 256),
		nn.ReLU(),
		nn.Linear(256, 10)
)
```

### 5.1.1.自定义块

**块必须提供的基本功能：**
1. 接收输入数据为前向传播的输入参数
2. 通过前向传播函数生成块的输出
3. 能够通过反向传播计算与访问输入参数的梯度（一般是自动进行）
4. 能够存储和访问前向传播所需要的参数
5. 提供可选初始化参数的功能

```python
class MLP(nn.Module):
    # 用模型参数声明层。这里，我们声明两个全连接的层
    def __init__(self):
        # 调用MLP的父类Module的构造函数来执行必要的初始化。
        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）
        super().__init__()
        self.hidden = nn.Linear(20, 256)  # 隐藏层
        self.out = nn.Linear(256, 10)  # 输出层

    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出
    def forward(self, X):
        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。
        return self.out(F.relu(self.hidden(X)))

net = MLP()
net(X)
```

### 5.1.2 顺序块

自定义顺序块（MySequential）：
- 将块按顺序添加到列表中的函数
- 将输入参数按照列表顺序依次传入各个块的传播函数

```python
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for idx, module in enumerate(args):
            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员
            # 变量_modules中。_module的类型是OrderedDict
            # 同时初始化参数时，初始化方法会自动找到_modules中的各个层的参数
            self._modules[str(idx)] = module

    def forward(self, X):
        # OrderedDict保证了按照成员添加的顺序遍历它们
        for block in self._modules.values():
            X = block(X)
        return X
```

### 5.1.3.在前向传播函数中执行代码

将参数的`requires_grad=False`设置为False从而关闭梯度计算，一直保持常熟
```python
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        # 不计算梯度的随机权重参数。因此其在训练期间保持不变
        self.rand_weight = torch.rand((20, 20), requires_grad=False)
        self.linear = nn.Linear(20, 20)

    def forward(self, X):
        X = self.linear(X)
        # 使用创建的常量参数以及relu和mm函数
        X = F.relu(torch.mm(X, self.rand_weight) + 1)
        # 复用全连接层。这相当于两个全连接层共享参数
        X = self.linear(X)
        # 控制流
        while X.abs().sum() > 1:
            X /= 2
        return X.sum()
```

### 5.1.4.效率

Python的问题[全局解释器锁](https://wiki.python.org/moin/GlobalInterpreterLock) 是众所周知的。 在深度学习环境中，我们担心速度极快的GPU可能要等到CPU运行Python代码后才能运行另一个作业

### 5.1.5 小结
- 一个块可以由多个层组成；一个块也可以由多个块组成
- 块可以包含代码
- 块中包含了大量内部处理，包括参数初始化和反向传播
- 层和块的顺序连接由Sequential块处理

## 5.2 参数管理

- 访问参数，用于调试、诊断和可视化
- 参数可视化
- 在不同模型组件间共享参数

### 5.2.1 参数访问

```python
net[2].state_dict()

# output
OrderedDict([
	('weight', tensor([[-0.0427, -0.2939, -0.1894,  0.0220, -0.1709, -0.1522, -0.0334, -0.2263]])), 
	('bias', tensor([0.0887]))
])
```

#### 5.2.1.1 目标参数

每个参数都表示为参数类的一个实例

```python
print(type(net[2].bias))
print(net[2].bias)
print(net[2].bias.data)

# output
<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([0.0887], requires_grad=True)
tensor([0.0887])
```

#### 5.2.1.2 一次性访问所有参数

```python
print(*[(name, param.shape) for name, param in net[0].named_parameters()])
print(*[(name, param.shape) for name, param in net.named_parameters()])

# output
('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))
('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))
```

另一种访问网络参数的形式

```python
net.state_dict()['2.bias'].data

# output
tensor([0.0887])
```

#### 5.2.1.3 从嵌套块收集参数

```python
def block1():
    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
                         nn.Linear(8, 4), nn.ReLU())

def block2():
    net = nn.Sequential()
    for i in range(4):
        # 在这里嵌套
        net.add_module(f'block {i}', block1())
    return net

rgnet = nn.Sequential(block2(), nn.Linear(4, 1))
rgnet

# output
Sequential(
  (0): Sequential(
    (block 0): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 1): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 2): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 3): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
  )
  (1): Linear(in_features=4, out_features=1, bias=True)
)
```

由于网络是分层嵌套，所有可以通过嵌套列表索引一样访问参数

```python
# 访问第一个块中第二个子块的第一层的偏置项
rgnet[0][1][0].bias.data

# output
tensor([ 0.1999, -0.4073, -0.1200, -0.2033, -0.1573,  0.3546, -0.2141, -0.2483])
```

### 5.2.2 参数初始化

默认情况下，PyTorch会根据一个范围均匀地初始化权重和偏置矩阵， 这个范围是根据输入和输出维度计算出的。 PyTorch的`nn.init`模块提供了多种预置初始化方法。

#### ### 5.2.2.1. 内置初始化

- 将所有权重参数初始化为标准差为0.01的高斯随机变量， 且将偏置参数设置为0：

```python
def init_normal(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, mean=0, std=0.01)
        nn.init.zeros_(m.bias)
net.apply(init_normal)
net[0].weight.data[0], net[0].bias.data[0]
```

- 将参数初始化为给定常数：

```python
def init_constant(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 1)
        nn.init.zeros_(m.bias)
net.apply(init_constant)
net[0].weight.data[0], net[0].bias.data[0]
```

- 对不同块应用不同的初始化方法：

```python
def init_xavier(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)
def init_42(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 42)

net[0].apply(init_xavier)
net[2].apply(init_42)
```

- 自定义初始化：![[Pasted image 20231031212005.png]]

```python
def my_init(m):
    if type(m) == nn.Linear:
        print("Init", *[(name, param.shape)
                        for name, param in m.named_parameters()][0])
        nn.init.uniform_(m.weight, -10, 10)
        m.weight.data *= m.weight.data.abs() >= 5

net.apply(my_init)
net[0].weight[:2]
```

- 手动直接设置参数

```python
net[0].weight.data[:] += 1
net[0].weight.data[0, 0] = 42
net[0].weight.data[0]
```

### 5.2.3 参数绑定

- 多个层间共享参数：定义一个稠密层，使用其参数来设置另一个层的参数

```python
# 我们需要给共享层一个名称，以便可以引用它的参数
shared = nn.Linear(8, 8)
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
                    shared, nn.ReLU(),
                    shared, nn.ReLU(),
                    nn.Linear(8, 1))
net(X)
# 检查参数是否相同
print(net[2].weight.data[0] == net[4].weight.data[0])
net[2].weight.data[0, 0] = 100
# 确保它们实际上是同一个对象，而不只是有相同的值
print(net[2].weight.data[0] == net[4].weight.data[0])

# output
tensor([True, True, True, True, True, True, True, True])
tensor([True, True, True, True, True, True, True, True])
```

### 5.2.4. 小结

- 我们有几种方法可以访问、初始化和绑定模型参数。
    
- 我们可以使用自定义初始化方法。

## 5.4 自定义层

### 5.4.1 无参数层

```python
class CenteredLayer(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, X):
        return X - X.mean()
```

### 5.4.2 带参数层

```python
class MyLinear(nn.Module):
    def __init__(self, in_units, units):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units, units))
        self.bias = nn.Parameter(torch.randn(units,))
    def forward(self, X):
        linear = torch.matmul(X, self.weight.data) + self.bias.data
        return F.relu(linear)
        
# 同样可以访问模型参数
# 注意这里的参数是通过继承父类中的nn.Parameter()方法而来的
linear = MyLinear(5, 3)
linear.weight
```
## 5.5 读写文件

### 5.5.1 加载和保存张量

- 单个张量的保存与读取：
```python
# 保存
x = torch.arange(4)
torch.save(x, 'x-file')

# 读取
x2 = torch.load('x-file')
```

- 张量列表的保存与读取
```python
y = torch.zeros(4)
torch.save([x, y],'x-files')
x2, y2 = torch.load('x-files')
```

- 张量字典的保存与读取
```python
mydict = {'x': x, 'y': y}
torch.save(mydict, 'mydict')
mydict2 = torch.load('mydict')
```

### 5.5.2 加载和保存模型参数

- 保存和加载模型参数：
```python
# 保存参数
torch.save(net.state_dict(), 'mlp.params')

# 加载参数
clone = MLP()
clone.load_state_dict(torch.load('mlp.params'))
```

### 5.5.3 小结

- `save`和`load`函数可用于张量对象的文件读写。
    
- 我们可以通过参数字典保存和加载网络的全部参数。
    
- 保存架构必须在代码中完成，而不是在参数中完成

## 5.6 GPU

```shell
# 查看显卡信息
!nvidia-smi
```

### 5.6.1 计算设备

- 利用`device`指定设备：
```python
torch.device('cpu'), torch.device('cuda'), torch.device('cuda:1')
```
- 查询可用GPU数量：`torch.cuda.device_count()`
- 方便函数：
```python
def try_gpu(i=0):  #@save
    """如果存在，则返回gpu(i)，否则返回cpu()"""
    if torch.cuda.device_count() >= i + 1:
        return torch.device(f'cuda:{i}')
    return torch.device('cpu')

def try_all_gpus():  #@save
    """返回所有可用的GPU，如果没有GPU，则返回[cpu(),]"""
    devices = [torch.device(f'cuda:{i}')
             for i in range(torch.cuda.device_count())]
    return devices if devices else [torch.device('cpu')]
```

### 5.6.2 张量与GPU

结合[[quick-leanr_PyTorch]]

```python
# 查询变量在什么设备上
x = torch.tensor([1, 2, 3])
x.device

Y = torch.rand(2, 3, device=try_gpu(1))
```

---

# 6.卷积神经网络

convolutional neural network

**图像识别、目标检测或语义分割**

卷积神经网络需要的参数少于全连接架构的网络，而且卷积也很容易用GPU并行计算。因此卷积神经网络除了能够高效地采样从而获得精确的模型，还能够高效地计算。

基本元素：
- 卷积层
- 填充 padding
- 步幅 stride
- 汇聚层/池化层 pooling
- 多通道 channel

- 现代卷积网络架构
- LeNet

## 6.1 全连接层到卷积

## 6.2 图像卷积

## 6.3 填充和步幅

## 6.4 多输入和多输出通道

## 6.5 汇聚层

## 6.6 卷积神经网络（LeNet）

---

# 7.现代卷积神经网络

## 7.1. 深度卷积神经网络（AlexNet）

LeNet在小数据集上取得了很好的效果

训练神经网络的一些关键技巧：
- **启发式参数初始化**
- **随机梯度下降的变体**
- **非挤压激活函数**
- **有效的正则化技术**

**特征提取算法**：
- SIFT（尺度不变特征变换） ([Lowe, 2004](https://zh-v2.d2l.ai/chapter_references/zreferences.html#id102 "Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. International journal of computer vision, 60(2), 91–110."))
- SURF（加速鲁棒特征） ([Bay _et al._, 2006](https://zh-v2.d2l.ai/chapter_references/zreferences.html#id7 "Bay, H., Tuytelaars, T., & Van Gool, L. (2006). Surf: speeded up robust features. European conference on computer vision (pp. 404–417)."))
- HOG（定向梯度直方图） ([Dalal and Triggs, 2005](https://zh-v2.d2l.ai/chapter_references/zreferences.html#id29 "Dalal, N., & Triggs, B. (2005). Histograms of oriented gradients for human detection. 2005 IEEE computer society conference on computer vision and pattern recognition (CVPR'05) (pp. 886–893)."))
- [bags of visual words](https://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision)
### 7.1.1. 学习表征

Alex Krizhevsky、Ilya Sutskever和Geoff Hinton提出了一种新的卷积神经网络变体_AlexNet_。在2012年ImageNet挑战赛中取得了轰动一时的成绩。

深度卷积神经网络的突破出现在2012年。突破可归因于两个关键因素。

#### 7.1.1.1. 缺少的成分：数据

包含许多特征的深度模型需要**大量的有标签数据**，才能显著优于**基于凸优化的传统方法**（如线性方法和核方法）

- 数据集的体量与质量
	- 在2010年前后兴起的大数据浪潮中得到改善
	- 

- 2009年，**ImageNet**数据集发布，并发起ImageNet挑战赛：要求研究人员从100万个样本中训练模型，以区分1000个不同类别的对象。
	- 
	- 
	- ImageNet数据集由斯坦福教授李飞飞小组的研究人员开发，利用谷歌图像搜索（Google Image Search）对每一类图像进行预筛选，并利用亚马逊众包（Amazon Mechanical Turk）来标注每张图片的相关类别。
#### 7.1.1.2. 缺少的成分：硬件

深度学习对计算资源要求很高，在20世纪90年代至21世纪初，优化凸目标的简单算法是研究人员的首选。

GPU相比于CPU：
- 核心数量更多的GPU比CPU快几个数量级
- GPU内核简单，更节能
- GPU拥有10倍于CPU的带宽

[cuda-convnet](https://code.google.com/archive/p/cuda-convnet/)：卷积和矩阵乘法，都是可以在硬件上并行化的操作
- Alex Krizhevsky和Ilya Sutskever使用两个显存为3GB的NVIDIA GTX580 GPU实现了快速卷积运算
### 7.1.2. AlexNet
![[Pasted image 20231102103557.png]]
- AlexNet比相对**较小**的LeNet5要**深**得多
- AlexNet使用**ReLU**而不是sigmoid作为其激活函数
#### 7.1.2.1. 模型设计

这里是精简版本的AlexNet，去除了当年需要两个小型GPU同时运算的设计特点

- 第一层：
	- 卷积窗口：11x11
		- ImageNet中大多数图像的宽和高比MNIST图像的多10倍以上，因此，需要一个更大的卷积窗口来捕获目标

#### 7.1.2.2. 激活函数

- **ReLU激活函数**
	- 计算更简单
	- 不同参数初始化方法，训练模型更容易
	- 防止梯度消失
#### 7.1.2.3. 容量控制和预处理

- AlexNet通过**暂退法（Dropout）** 控制全连接层的模型复杂度，而LeNet只使用了权重衰减
- 增加了大量的图像增强数据，如翻转、裁切和变色


## 7.2. 使用块的网络（VGG）

使用块的想法首先出现在牛津大学的[视觉几何组（visual geometry group）](http://www.robots.ox.ac.uk/~vgg/)的_VGG网络_中。通过使用循环和子程序，可以很容易地在任何现代深度学习框架的代码中实现这些重复的架构。

### 7.2.1. VGG块

