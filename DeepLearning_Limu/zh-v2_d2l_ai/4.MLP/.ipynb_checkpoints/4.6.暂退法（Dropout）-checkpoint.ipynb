{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "312d4f17-7fef-487b-a414-2aa477da02d8",
   "metadata": {},
   "source": [
    "## 4.6. 暂退法（Dropout）\n",
    "\n",
    "希望模型深度挖掘特征，即将其权重分散到许多特征中， 而不是过于依赖少数潜在的虚假关联。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c895754-2974-4979-acfe-8baddb6237d4",
   "metadata": {},
   "source": [
    "### 4.6.1. 重新审视过拟合\n",
    "\n",
    "泛化性和灵活性之间的这种基本权衡被描述为偏差-方差权衡（bias-variance tradeoff）\n",
    "\n",
    "线性模型有很高的偏差：它们**只能表示一小类函数**。然而这些模型的方差很低：它们在**不同的随机数据**样本上可以得出**相似**的结果\n",
    "\n",
    "深度神经网络位于偏差-方差谱的另一端.神经网络并不局限于单独查看每个特征，而是**学习特征之间的交互**\n",
    "\n",
    "**深度网络的泛化性质令人费解，而这种泛化性质的数学基础仍然是悬而未决的研究问题。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2e4913-b9b6-41a0-8e13-9873fd7d7634",
   "metadata": {},
   "source": [
    "### 4.6.2. 扰动的稳健性\n",
    "\n",
    "“好”的预测模型能在未知的数据上有很好的表现： 经典泛化理论认为，为了缩小训练和测试性能之间的差距，应该以简单的模型为目标。*简单性以较小维度的形式展现*\n",
    "\n",
    "简单性的另一个角度是**平滑性**，即**函数不应该对其输入的微小变化敏感**。\n",
    "\n",
    "1995年，克里斯托弗·毕晓普证明了 具有输入噪声的训练等价于Tikhonov正则化,用数学证实了“要求函数光滑”和“要求函数对输入的随机噪声具有适应性”之间的联系。[Bishop, 1995](https://doi.org/10.1162/neco.1995.7.1.108)\n",
    "在2014年，斯里瓦斯塔瓦等人 ([Srivastava et al., 2014](https://www.researchgate.net/publication/286794765_Dropout_A_Simple_Way_to_Prevent_Neural_Networks_from_Overfitting)) 就如何将毕晓普的想法应用于网络的内部层提出了一个想法： 在训练过程中，他们建议在计算后续层之前向网络的每一层注入噪声。 因为当训练一个有多层的深层网络时，注入噪声只会在输入-输出映射上增强平滑性。\n",
    "\n",
    "**暂退法（dropout）**：前向传播中，在每一层的输入中增加噪声。\n",
    "从表面上看是在训练过程中丢弃（drop out）一些神经元。 在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。\n",
    "\n",
    "**如何注入这种噪声**：一种无偏向（unbiased）的方式注入噪声。以概率$p$将节点活性值$h$用$h'$代替，其中$\\mathrm{P}(h'=0)=p,\\ \\mathrm{P}(h'=\\frac{h}{1-p})=1-p$。计算期望值可得：$E[h']=h$，所以此种注入噪声方式可以保证其期望值不发生改变。\n",
    "\n",
    "**根据指定的暂退概率随机丢弃上一层的输出（相当于下一层的输入）**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7cac7f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 4.6.3.实践中的暂退法\n",
    "\n",
    "通常，我们**在测试时不用暂退法**。 给定一个训练好的模型和一个新的样本，我们不会丢弃任何节点，因此不需要标准化。\n",
    "\n",
    "例外：一些研究人员在测试时使用暂退法， 用于估计神经网络预测的“不确定性”： 如果通过许多不同的暂退法遮盖后得到的预测结果都是一致的，那么我们可以说网络发挥更稳定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b7c3312",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def dropout_layer(X, dropout):\n",
    "    assert 0 <= dropout <= 1\n",
    "    # 在本情况中，所有元素都被丢弃\n",
    "    if dropout == 1:\n",
    "        return torch.zeros_like(X)\n",
    "    # 在本情况中，所有元素都被保留\n",
    "    if dropout == 0:\n",
    "        return X\n",
    "    mask = (torch.rand(X.shape) > dropout).float() # 实现dropout的关键\n",
    "    return mask * X / (1.0 - dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f358108f-9454-4b7b-bacf-961d6aa393d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])\n",
      "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])\n",
      "tensor([[ 0.,  0.,  0.,  0.,  0.,  0., 12.,  0.],\n",
      "        [ 0.,  0., 20.,  0.,  0.,  0., 28., 30.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "X= torch.arange(16, dtype = torch.float32).reshape((2, 8))\n",
    "print(X)\n",
    "print(dropout_layer(X, 0.))\n",
    "print(dropout_layer(X, 0.5))\n",
    "print(dropout_layer(X, 1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4768a401-a0d4-44b6-89dd-5445fdd1e594",
   "metadata": {},
   "source": [
    "### 4.6.6. 小结\n",
    "\n",
    "暂退法在前向传播过程中，计算每一内部层的同时丢弃一些神经元。\r\n",
    "\r\n",
    "暂退法可以避免过拟合，它通常与控制权重向量的维数和大小结合使用的。\r\n",
    "\r\n",
    "暂退法$h$活性值\r\n",
    "替换为$h$有期望值\r\n",
    "的随机变量。\r\n",
    "\r\n",
    "暂退法仅在训练期间使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a82589-d005-42cd-90c8-5f81c47f14a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffe893b9-32f4-46ea-b4ef-3fdcde66111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n",
    "\n",
    "dropout1, dropout2 = 0.2, 0.5\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,\n",
    "                 is_training = True):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.training = is_training\n",
    "        self.lin1 = nn.Linear(num_inputs, num_hiddens1)\n",
    "        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)\n",
    "        self.lin3 = nn.Linear(num_hiddens2, num_outputs)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs))))\n",
    "        # 只有在训练模型时才使用dropout\n",
    "        if self.training == True:\n",
    "            # 在第一个全连接层之后添加一个dropout层\n",
    "            H1 = dropout_layer(H1, dropout1)\n",
    "        H2 = self.relu(self.lin2(H1))\n",
    "        if self.training == True:\n",
    "            # 在第二个全连接层之后添加一个dropout层\n",
    "            H2 = dropout_layer(H2, dropout2)\n",
    "        out = self.lin3(H2)\n",
    "        return out\n",
    "\n",
    "net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5b80981-5a23-42ec-b8d3-24b44fb72224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------第 1 轮训练开始----------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 24\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     27\u001b[0m total_train_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m               \u001b[38;5;66;03m# 记录训练次数\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:244\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    235\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    236\u001b[0m     (inputs,)\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[0;32m    241\u001b[0m )\n\u001b[0;32m    243\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[1;32m--> 244\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:117\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 117\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    118\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    119\u001b[0m         )\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[0;32m    121\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    122\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m         )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "num_epochs, lr, batch_size = 10, 0.5, 256\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "total_train_step = 0    # 记录训练次数\n",
    "total_test_step = 0     # 记录测试次数\n",
    "\n",
    "writer = SummaryWriter(\"./logs_trains\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"----------第 {} 轮训练开始----------\".format(epoch+1))\n",
    "    \n",
    "    net.train()\n",
    "    for data in train_iter:\n",
    "        imgs, labels = data\n",
    "        outputs = net(imgs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_step += 1               # 记录训练次数\n",
    "        if total_train_step % 100 == 0:\n",
    "            print(\"训练次数：{}, loss={}\".format(total_train_step, loss.item()))\n",
    "            writer.add_scalar(\"train_loss\", loss.item(), total_train_step)\n",
    "\n",
    "    # 测试开始\n",
    "    nte.eval()   # 设置模型为测试模式（只对部分层有效）\n",
    "    total_test_loss = 0\n",
    "    total_test_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_iter:\n",
    "            imgs, labels = data\n",
    "            outputs = net(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_test_loss += loss\n",
    "            accuracy = (outputs.argmax(1) == labels).sum()\n",
    "            total_test_accuracy += accuracy\n",
    "\n",
    "    print(\"整体测试集上的Loss：{}\".format(total_test_loss))\n",
    "    print(\"整体测试集上的正确率：{}\".format(total_test_accuracy/test_data_size))\n",
    "    writer.add_scalar(\"test_loss\", total_test_loss, total_test_step)\n",
    "    writer.add_scalar(\"test_accu\", total_test_accuracy/test_data_size, total_test_step)\n",
    "    total_test_step += 1\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16fa4a1-471e-45b3-91a3-be8a107d9bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
