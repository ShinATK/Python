{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "回归可以用于预测多少的问题，事实上，我们也对分类问题感兴趣：不是问“多少”，而是问“哪一个”\n",
    "\n",
    "机器学习实践者用分类这个词来描述两个有微妙差别的问题：\n",
    "1. 我们只对样本的“硬性”类别感兴趣，即属于哪个类别；\n",
    "2. 我们希望得到“软性”类别，即得到属于每个类别的概率。\n",
    "\n",
    "一般的分类问题并不与类别之间的自然顺序有关，无法按照顺序进行1234...编码\n",
    "\n",
    "独热编码（one-hot encoding）。 独热编码是一个向量，它的分量和类别一样多。 类别对应的分量设置为1，其他所有分量设置为0。\n",
    "\n",
    " softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质。\n",
    " \n",
    " $$\\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o}),\\ \\hat{y_j}=\\frac{\\exp(o_j)}{\\sum_k\\exp(o_k)}$$\n",
    "$$\\mathrm{argmax} \\hat{y}_j = \\mathrm{argmax} o_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全连接层的参数开销\n",
    "正如我们将在后续章节中看到的，在深度学习中，全连接层无处不在。 然而，顾名思义，全连接层是“完全”连接的，可能有很多可学习的参数。 具体来说，对于任何具有$d$\n",
    "个输入$q$\r\n",
    "个输出的全连，， 参数开$\\mathrm{O}(dq)$为\r\n",
    "，这个数字在实践中可能高得令人望而步。 幸运的$d$，将\r\n",
    "个输$q$转换为\r\n",
    "个输出的成本$\\mathrm{O}(\\frac{dq}{n})$以少\r\n",
    " \r\n",
    "$n$ 其中超参数\r\n",
    "可以以在实际应用中平衡参数节约和模型有效性[和模型有效性 (Zhanget a](https://arxiv.org/abs/2102.08597).02.08597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
